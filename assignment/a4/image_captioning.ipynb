{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c15d11c1",
      "metadata": {},
      "source": [
        "# Image Captioning\n",
        "\n",
        "This assignment is somewhat short.  We want you to spend your time on the project instead!\n",
        "\n",
        "This assignment explores models connecting different modalities - exploring a connection between images and text.  By the time you're done this assignment, you'll:\n",
        "\n",
        "* explored MS COCO captioning dataset\n",
        "* investigated a few captioning techniques\n",
        "* played with an image captioner\n",
        "\n",
        "\n",
        "### Data\n",
        "\n",
        "* Download the 2014 validation images, as well as their annotations from https://cocodataset.org/#download.  On a os x or linux, a command like ```curl http://images.cocodataset.org/zips/val2014.zip --output val2014.zip``` for each will do what you need.\n",
        "* Unzip both files and name the corresponding directories `val2014` and `annotations` in this assignment directory.\n",
        "\n",
        "### Explore the dataset\n",
        "Look in the annotations directory.  Which file(s) contain the image captions?  Load those into memory here.  Your end goal is to generate a list of tuples for each of train2014 (you don't need the actual images to do this!) and val2014 i.e. ```[(391895, 'val2014/COCO_val2014_000000391895.jpg', 'A man with a red helmet on a small moped on a dirt road. '), ...]```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2ce9969e",
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "import json\n",
        "\n",
        "train2014 = []\n",
        "val2014 = []\n",
        "    \n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0b9e96f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "assert (479495,\n",
        "  'train2014/COCO_train2014_000000479495.jpg',\n",
        "  'A bicycle is parked by a bench at night.') in train2014\n",
        "assert (203564,\n",
        "  'val2014/COCO_val2014_000000203564.jpg',\n",
        "  'A black metal bicycle with a clock inside the front wheel.') in val2014"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "eebff9a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell if you want to write code to answer the questions below.\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da525fcc",
      "metadata": {},
      "source": [
        "### Questions (Part A)\n",
        "\n",
        "1. How many images do we have captions for in the training set?\n",
        "2. How many captions do we have per image (rounded to nearest integer)?\n",
        "3. If you just split on whitespace and do nothing else, how many words are in the vocabulary?\n",
        "\n",
        "Given your answer to 3, think about what you might need to do with that few examples and that large of a vocabulary?  (Use pretrained embeddings from a massive semi-supervised dataset, keeping only the top-k tokens, smarter tokenization, ...).  You don't need to write your answer anywhere, but given how often these problems arise in NLP, you should be feeling more confident at this point in the course how to handle these situations!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44b656dc",
      "metadata": {},
      "source": [
        "## Let's look at some of the examples!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "53f9faea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "import random\n",
        "\n",
        "samples = 5\n",
        "n = 1\n",
        "fig = plt.figure(figsize=(10,20))\n",
        "for i in range(samples):\n",
        "    image_caption = val2014[random.randint(0, len(val2014))]\n",
        "    image_load = load_img(image_caption[1])\n",
        "    \n",
        "    ax = fig.add_subplot(samples,2,n,xticks=[],yticks=[])\n",
        "    ax.imshow(image_load)\n",
        "    n += 1\n",
        "    \n",
        "    ax = fig.add_subplot(samples,2,n)\n",
        "    plt.axis('off')\n",
        "    ax.plot()\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.text(0, 0, image_caption[2], fontsize=20)\n",
        "    n += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c84fd83",
      "metadata": {},
      "source": [
        "# Foundational image captioning papers\n",
        "\n",
        "## Show & Tell\n",
        "\n",
        "[Show and Tell: A Neural Image Caption Generator](https://arxiv.org/pdf/1411.4555.pdf) was the first step towards neural image captioning.  Fundamentally it is a encoder-decoder scheme similar to what we've seen in class.  Concretely, it uses the CNN structure of a (at the time) state of the art image classification CNN as the encoder and it uses a LSTM as a decoder.  As in the generation models in class, it continues to generate text until a special \"stop\" token is emitted.  After **reading** the paper, answer the following questions:\n",
        "\n",
        "### Questions (Part B)\n",
        "\n",
        "1.  What parts of the CNN were trained?\n",
        "2.  What was the biggest conern when deciding?\n",
        "3.  How was the encoded image representation input into the decoder?\n",
        "4.  Given we are \"translating\" from an image to a caption (without a length constraint), which evaluation metric did the authors determine was reasonable for a top line metric?\n",
        "5.  What beam width is equivalent to one where you select the highest probability word in each decoding step?\n",
        "6.  How many points of quality did using a beam search of 20 provide versus a greedy search?\n",
        "7.  Where did the authors get their word embeddings from?\n",
        "8.  Approximately how often were there novel sentences in the top-15 generated text candidates, despite overfitting issues?\n",
        "\n",
        "## Deep visual alignment\n",
        "\n",
        "[Deep Visual-Semantic Alignments for Generating Image Descriptions](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf) is a fun read for which we will ask no questions.  Its critical insights are around understanding an image as a composition of regions, and building upon that understanding to construct both a caption for the whole image, but labels for its consistuent parts.\n",
        "\n",
        "## Show, Attend & Tell\n",
        "\n",
        "[Show, Attend & Tell](https://arxiv.org/pdf/1502.03044.pdf) applies the same \"provide the decoder more context, as directly as possible\" trick we've seen over the course: adding attention.  After **skimming** the paper, answer the following questions:\n",
        "\n",
        "### Questions (Part C)\n",
        "\n",
        "1. What is the attention over?\n",
        "2. What do the figures with highlight shading represent in Figures 2, 3 and 5?\n",
        "\n",
        "# Exploring a MS COCO captioner\n",
        "\n",
        "There are many examples of image captioners ML engineers have built on the MS COCO dataset you explored. [This one](https://replicate.com/rmokady/clip_prefix_caption) uses a (more) modern large language model as its decoder, GPT-2.  \n",
        "\n",
        "* **Explore** the samples and play with using beam search and not.  What do you notice?\n",
        "\n",
        "This is an example from the Show & Tell paper of a low-quality caption (see figure 5).  The GPT-2 model proposes \"the car that person drove to the hospital.\" vs. \"A yellow school bus parked in a parking lot\" from the original paper. ![Misclassified](littlecar.png) "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
