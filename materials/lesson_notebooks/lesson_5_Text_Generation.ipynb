{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CJVpUxEKnCn"
   },
   "source": [
    "## Lesson Notebook 5: Text Generation\n",
    "\n",
    "In this notebook we will look at 2 components:\n",
    "\n",
    "1. Buliding a Seq2Seq model for Translation using RNNs with and without Attention\n",
    "\n",
    "2. Playing with T5\n",
    "\n",
    "Part 1 is inspired by the Keras Tutorial https://keras.io/examples/nlp/lstm_seq2seq/.\n",
    "\n",
    "We first need to do the usual setup. We will also use some nltk and sklearn components in order to tokenize the text.\n",
    "\n",
    "\n",
    "  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2022-summer-main/blob/master/materials/lesson_notebooks/lesson_5_Text_Generation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oKdHlKOSKkgM"
   },
   "outputs": [],
   "source": [
    "#@title Installs\n",
    "\n",
    "!pip install pydot --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install sentencepiece --quiet\n",
    "!pip install nltk --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FgTghneFvZqd"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration\n",
    "\n",
    "\n",
    "import sklearn as sk\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhvbHp3Gq7CM",
    "outputId": "03adc8ec-e362-403d-e718-1897425f92f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM3-mYDnnxyT"
   },
   "source": [
    "### 1. Buliding a Seq2Seq model for Translation using RNNs with and without Attention\n",
    "\n",
    "#### 1.a Downloading and pre-processing Data\n",
    "\n",
    "\n",
    "Let's get the data. Just like the Keras tutorial, we will use http://www.manythings.org as the source for the parallel corpus, but we will use German: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJgys8FQva6V",
    "outputId": "faffa63e-2870-418e-d48e-e12dff4fea61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archive:  deu-eng.zip',\n",
       " 'replace deu.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y',\n",
       " '  inflating: deu.txt                 ',\n",
       " 'replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y',\n",
       " '  inflating: _about.txt              ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!curl -O http://www.manythings.org/anki/deu-eng.zip\n",
    "!!unzip deu-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fGnUBkZQteK"
   },
   "source": [
    "Next, we need to set a few parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mWyF80SsnP-M"
   },
   "outputs": [],
   "source": [
    "embed_dim = 100  # Embedding dimensions for vectors and LSTMs.\n",
    "num_samples = 10000  # Number of examples to consider.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = \"deu.txt\"\n",
    "\n",
    "# Vocabulary sizes that we consider:\n",
    "english_vocab_size = 2000\n",
    "german_vocab_size = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgRfFYZsRGmn"
   },
   "source": [
    "Next, we need to format the input. In particular we would like to use nltk to help with the tokenization. We will then use sklearn to perform the counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSTzz6VAtlqv",
    "outputId": "23e91251-06e3-4a8a-ca2f-c91280c29951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum source input length:  6\n",
      "Maximum target output length:  11\n"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "max_input_length = -1\n",
    "max_output_length = -1\n",
    "\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "\n",
    "    tokenized_source_text = nltk.word_tokenize(input_text, language='english')\n",
    "    tokenized_target_text = nltk.word_tokenize(target_text, language='german')\n",
    "\n",
    "    if len(tokenized_source_text) > max_input_length:\n",
    "      max_input_length = len(tokenized_source_text)\n",
    "\n",
    "    if len(tokenized_target_text) > max_output_length:\n",
    "      max_output_length = len(tokenized_target_text)\n",
    "\n",
    "\n",
    "    source_text = (' '.join(tokenized_source_text)).lower()\n",
    "    target_text = (' '.join(tokenized_target_text)).lower()\n",
    "\n",
    "    input_texts.append(source_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "  \n",
    "vect_english = CountVectorizer(max_features=english_vocab_size)\n",
    "vect_german = CountVectorizer(max_features=german_vocab_size)\n",
    "\n",
    "vectorized_english_input = vect_english.fit_transform(input_texts)\n",
    "vectorized_german_target = vect_german.fit_transform(target_texts)\n",
    "\n",
    "print('Maximum source input length: ', max_input_length)\n",
    "print('Maximum target output length: ', max_output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQwEsJh7R8Yd",
    "outputId": "6807dea1-d59b-4e06-a234-834153ac0aca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go .', 'hi .']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uz463u9GR_wO",
    "outputId": "9ec9c7c5-0061-4252-c4a3-7b5b02073fb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geh .', 'hallo !']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIAaknDaRasy"
   },
   "source": [
    "Looks simple but correct.\n",
    "\n",
    "So the source and target sequences have max lengths 6 and 11, respectively. As we will add start and end tokens to our decoder side we will set the respective max lengths to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mDdzDfvQ9EO1"
   },
   "outputs": [],
   "source": [
    "max_encoder_seq_length = 6\n",
    "max_decoder_seq_length = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtF9ihIeRuTO"
   },
   "source": [
    "Next, we create the dictionaries translating between ids and tokens for both source (English) and target (German)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8JiJ9pL3AnU",
    "outputId": "c1fe1a58-a014-4efd-e7e3-818402c41587"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "sid_svocab_dict = {}\n",
    "svocab_sid_dict = {}\n",
    "\n",
    "for sid, svocab in enumerate(vect_english.get_feature_names()):\n",
    "  sid_svocab_dict[sid] = svocab\n",
    "  svocab_sid_dict[svocab] = sid\n",
    "\n",
    "sid_svocab_dict[english_vocab_size] = \"<unk>\"\n",
    "sid_svocab_dict[english_vocab_size + 1] = \"<pad>\"\n",
    "\n",
    "svocab_sid_dict[\"<unk>\"] = english_vocab_size\n",
    "svocab_sid_dict[\"<pad>\"] = english_vocab_size + 1\n",
    "\n",
    "tid_tvocab_dict = {}\n",
    "tvocab_tid_dict = {}\n",
    "\n",
    "for tid, tvocab in enumerate(vect_german.get_feature_names()):\n",
    "  tid_tvocab_dict[tid] = tvocab\n",
    "  tvocab_tid_dict[tvocab] = tid\n",
    "\n",
    "# Add unknown token plus start and end tokens to target language\n",
    "\n",
    "tid_tvocab_dict[german_vocab_size] = \"<unk>\"\n",
    "tid_tvocab_dict[german_vocab_size + 1] = \"<start>\"\n",
    "tid_tvocab_dict[german_vocab_size + 2] = \"<end>\"\n",
    "tid_tvocab_dict[german_vocab_size + 3] = \"<pad>\"\n",
    "\n",
    "tvocab_tid_dict[\"<unk>\"] = german_vocab_size\n",
    "tvocab_tid_dict[\"<start>\"] = german_vocab_size + 1\n",
    "tvocab_tid_dict[\"<end>\"] = german_vocab_size + 2\n",
    "tvocab_tid_dict[\"<pad>\"] = german_vocab_size + 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKY10hN1SLui"
   },
   "source": [
    "Lastly, we need to create the training and test data that will feed into our two models. It is convenient to define a small function for that that also takes care off padding and adding start/end tokens on the decoder side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DWf2bb905SW9"
   },
   "outputs": [],
   "source": [
    "def convert_text_to_date(texts, \n",
    "                         vocab_id_dict, \n",
    "                         max_length=20, \n",
    "                         type=None,\n",
    "                         train_test_vector=None,\n",
    "                         samples=100000):\n",
    "\n",
    "  \n",
    "  if type == None:\n",
    "    raise ValueError('\\'type\\' is not defined. Please choose from: input_source, input_target, output_target.')\n",
    "  \n",
    "  train_data = []\n",
    "  test_data = []\n",
    "\n",
    "  for text_num, text in enumerate(texts[:samples]):\n",
    "\n",
    "    sentence_ids = []\n",
    "\n",
    "    for token in text.split():\n",
    "\n",
    "      if token in vocab_id_dict.keys():\n",
    "        sentence_ids.append(vocab_id_dict[token])\n",
    "      else:\n",
    "        sentence_ids.append(vocab_id_dict[\"<unk>\"])\n",
    "    \n",
    "    vocab_size = len(vocab_id_dict.keys())\n",
    "    \n",
    "    # Depending on encoder/decoder and input/output, add start/end tokens.\n",
    "    # Then add padding.\n",
    "    \n",
    "    if type == 'input_source':\n",
    "      ids = (sentence_ids + [vocab_size - 1] * max_length)[:max_length]\n",
    "\n",
    "    elif type == 'input_target':\n",
    "      ids = ([vocab_size -3] + sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "\n",
    "    elif type == 'output_target':\n",
    "      ids = (sentence_ids + [vocab_size - 2] + [vocab_size -1] * max_length)[:max_length]\n",
    "\n",
    "    if train_test_vector is not None and not train_test_vector[text_num]:\n",
    "      test_data.append(ids)\n",
    "    else:\n",
    "      train_data.append(ids)\n",
    "\n",
    "\n",
    "  return np.array(train_data), np.array(test_data)\n",
    "\n",
    "\n",
    "train_test_split_vector = (np.random.uniform(size=10000) > 0.2)\n",
    "\n",
    "train_source_input_data, test_source_input_data = convert_text_to_date(input_texts, \n",
    "                                         svocab_sid_dict, \n",
    "                                         type='input_source',\n",
    "                                         max_length=max_encoder_seq_length,\n",
    "                                         train_test_vector=train_test_split_vector,\n",
    "                                         #samples=2\n",
    "                                         )\n",
    "\n",
    "train_target_input_data, test_target_input_data = convert_text_to_date(target_texts, \n",
    "                                         tvocab_tid_dict, \n",
    "                                         type='input_target',\n",
    "                                         max_length=max_decoder_seq_length,\n",
    "                                         train_test_vector=train_test_split_vector,\n",
    "                                         #samples=2\n",
    "                                         )\n",
    "\n",
    "train_target_output_data, test_target_output_data = convert_text_to_date(target_texts, \n",
    "                                         tvocab_tid_dict, \n",
    "                                         type='output_target',\n",
    "                                         max_length=max_decoder_seq_length,\n",
    "                                        train_test_vector=train_test_split_vector,\n",
    "                                         #samples=2\n",
    "                                          )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwsJeSJVWbnI"
   },
   "source": [
    "Let us look at a few examples. They appear coorect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YB5WqPpYV_EQ",
    "outputId": "2288cacf-90b4-47b7-df24-73aa5716896b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 848, 2000, 2001, 2001, 2001, 2001],\n",
       "       [ 848, 2000, 2001, 2001, 2001, 2001]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source_input_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N103xCPpWBx7",
    "outputId": "071a8f3b-06c8-4716-8101-940c1ee9612c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3001, 1244, 3000, 3002, 3003, 3003, 3003, 3003, 3003, 3003, 3003,\n",
       "        3003, 3003],\n",
       "       [3001, 3000, 1218, 3000, 3002, 3003, 3003, 3003, 3003, 3003, 3003,\n",
       "        3003, 3003]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_input_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7Wcc1bFWB5c",
    "outputId": "a4101377-5ba9-4f9d-8536-8fba2c530d8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1244, 3000, 3002, 3003, 3003, 3003, 3003, 3003, 3003, 3003, 3003,\n",
       "        3003, 3003],\n",
       "       [3000, 1218, 3000, 3002, 3003, 3003, 3003, 3003, 3003, 3003, 3003,\n",
       "        3003, 3003]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_output_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMTL8pWhSxjc"
   },
   "source": [
    "#### 1.b The Seq2seq model without Attention\n",
    "\n",
    "This is straightforward to build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UK9eP5bC6jCL"
   },
   "outputs": [],
   "source": [
    "encode_vocab_size = len(sid_svocab_dict.keys())\n",
    "decode_vocab_size = len(tid_tvocab_dict.keys())\n",
    "\n",
    "### Translation Model ###\n",
    "\n",
    "source_input_no_att = tf.keras.layers.Input(shape=(max_encoder_seq_length,), \n",
    "                                     dtype='int64',\n",
    "                                     name='source_input_no_att')\n",
    "target_input_no_att = tf.keras.layers.Input(shape=(max_decoder_seq_length,), \n",
    "                                     dtype='int64',\n",
    "                                     name='target_input_no_att')\n",
    "\n",
    "source_embedding_layer_no_att = tf.keras.layers.Embedding(input_dim=encode_vocab_size,\n",
    "                                              output_dim=embed_dim, \n",
    "                                              name='source_embedding_layer_no_att')\n",
    "\n",
    "target_embeddings_layer_no_att  = tf.keras.layers.Embedding(input_dim=decode_vocab_size,\n",
    "                                              output_dim=embed_dim, \n",
    "                                              name='target_embedding_layer_no_att')\n",
    "\n",
    "\n",
    "source_embeddings_no_att = source_embedding_layer_no_att(source_input_no_att)\n",
    "target_embeddings_no_att = target_embeddings_layer_no_att(target_input_no_att)\n",
    "\n",
    "encoder_lstm_layer_no_att = tf.keras.layers.LSTM(embed_dim, return_sequences=True, return_state=True, name='encoder_lstm_layer_no_att')\n",
    "encoder_out_no_att, encoder_state_h_no_att, encoder_state_c_no_att = encoder_lstm_layer_no_att(source_embeddings_no_att)\n",
    "\n",
    "\n",
    "decoder_lstm_layer_no_att = tf.keras.layers.LSTM(embed_dim, return_sequences=True, return_state=False, name='decoder_lstm_layer_no_att')\n",
    "decoder_lstm_out_no_att = decoder_lstm_layer_no_att(target_embeddings_no_att, [encoder_state_h_no_att, encoder_state_c_no_att])\n",
    "\n",
    "\n",
    "target_classification_no_att = tf.keras.layers.Dense(decode_vocab_size, \n",
    "                                              activation='softmax', \n",
    "                                              name='classification_no_att')(decoder_lstm_out_no_att)\n",
    "\n",
    "\n",
    "translation_model_no_att = tf.keras.models.Model(inputs=[source_input_no_att, target_input_no_att], outputs=[target_classification_no_att])\n",
    "\n",
    "\n",
    "translation_model_no_att.compile(optimizer=\"Adam\",\n",
    "                          loss='sparse_categorical_crossentropy', \n",
    "                          metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQafY_4y6jFl",
    "outputId": "7a6ac316-d1cb-4169-8827-b8c340d131ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " source_input_no_att (InputLaye  [(None, 6)]         0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " target_input_no_att (InputLaye  [(None, 13)]        0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " source_embedding_layer_no_att   (None, 6, 100)      200200      ['source_input_no_att[0][0]']    \n",
      " (Embedding)                                                                                      \n",
      "                                                                                                  \n",
      " target_embedding_layer_no_att   (None, 13, 100)     300400      ['target_input_no_att[0][0]']    \n",
      " (Embedding)                                                                                      \n",
      "                                                                                                  \n",
      " encoder_lstm_layer_no_att (LST  [(None, 6, 100),    80400       ['source_embedding_layer_no_att[0\n",
      " M)                              (None, 100),                    ][0]']                           \n",
      "                                 (None, 100)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm_layer_no_att (LST  (None, 13, 100)     80400       ['target_embedding_layer_no_att[0\n",
      " M)                                                              ][0]',                           \n",
      "                                                                  'encoder_lstm_layer_no_att[0][1]\n",
      "                                                                 ',                               \n",
      "                                                                  'encoder_lstm_layer_no_att[0][2]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " classification_no_att (Dense)  (None, 13, 3004)     303404      ['decoder_lstm_layer_no_att[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 964,804\n",
      "Trainable params: 964,804\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "translation_model_no_att.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w911c-QMYky3"
   },
   "source": [
    "It never hurts to look at the shapes of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-NVTE_pU3-H",
    "outputId": "40556667-ea6d-4a39-8211-e44443954f2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7998, 13, 3004)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model_no_att.predict(x=[train_source_input_data, train_target_input_data]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h35KVb-G6jNs",
    "outputId": "8e5bbc80-5f41-40c8-e919-3d56ad59ca0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 6s 13ms/step - loss: 2.5465 - accuracy: 0.6415 - val_loss: 1.7186 - val_accuracy: 0.7483\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 1.5724 - accuracy: 0.7628 - val_loss: 1.5157 - val_accuracy: 0.7710\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 1.4277 - accuracy: 0.7738 - val_loss: 1.4077 - val_accuracy: 0.7787\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 1.3206 - accuracy: 0.7866 - val_loss: 1.3234 - val_accuracy: 0.7925\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 1.2383 - accuracy: 0.7967 - val_loss: 1.2637 - val_accuracy: 0.8007\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 1.1629 - accuracy: 0.8072 - val_loss: 1.2069 - val_accuracy: 0.8093\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 1.0946 - accuracy: 0.8153 - val_loss: 1.1614 - val_accuracy: 0.8142\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 1.0357 - accuracy: 0.8220 - val_loss: 1.1281 - val_accuracy: 0.8206\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.9843 - accuracy: 0.8281 - val_loss: 1.0996 - val_accuracy: 0.8228\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.9380 - accuracy: 0.8323 - val_loss: 1.0769 - val_accuracy: 0.8257\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.8949 - accuracy: 0.8365 - val_loss: 1.0545 - val_accuracy: 0.8287\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.8542 - accuracy: 0.8424 - val_loss: 1.0395 - val_accuracy: 0.8336\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.8153 - accuracy: 0.8478 - val_loss: 1.0211 - val_accuracy: 0.8366\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.7773 - accuracy: 0.8526 - val_loss: 1.0058 - val_accuracy: 0.8391\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.7415 - accuracy: 0.8575 - val_loss: 0.9921 - val_accuracy: 0.8420\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.7078 - accuracy: 0.8616 - val_loss: 0.9758 - val_accuracy: 0.8447\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.6756 - accuracy: 0.8656 - val_loss: 0.9692 - val_accuracy: 0.8448\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.6440 - accuracy: 0.8699 - val_loss: 0.9607 - val_accuracy: 0.8472\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.6151 - accuracy: 0.8741 - val_loss: 0.9478 - val_accuracy: 0.8497\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.5863 - accuracy: 0.8777 - val_loss: 0.9432 - val_accuracy: 0.8498\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.5599 - accuracy: 0.8810 - val_loss: 0.9346 - val_accuracy: 0.8526\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.5342 - accuracy: 0.8849 - val_loss: 0.9325 - val_accuracy: 0.8524\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.5099 - accuracy: 0.8888 - val_loss: 0.9264 - val_accuracy: 0.8548\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.4860 - accuracy: 0.8929 - val_loss: 0.9233 - val_accuracy: 0.8545\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.4629 - accuracy: 0.8964 - val_loss: 0.9180 - val_accuracy: 0.8557\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.4407 - accuracy: 0.9004 - val_loss: 0.9216 - val_accuracy: 0.8554\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.4198 - accuracy: 0.9042 - val_loss: 0.9158 - val_accuracy: 0.8565\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.3996 - accuracy: 0.9079 - val_loss: 0.9148 - val_accuracy: 0.8566\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.3807 - accuracy: 0.9116 - val_loss: 0.9109 - val_accuracy: 0.8579\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.3625 - accuracy: 0.9157 - val_loss: 0.9213 - val_accuracy: 0.8564\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.3452 - accuracy: 0.9192 - val_loss: 0.9131 - val_accuracy: 0.8582\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.3280 - accuracy: 0.9229 - val_loss: 0.9093 - val_accuracy: 0.8609\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.3121 - accuracy: 0.9262 - val_loss: 0.9137 - val_accuracy: 0.8607\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.2969 - accuracy: 0.9296 - val_loss: 0.9141 - val_accuracy: 0.8616\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.2825 - accuracy: 0.9328 - val_loss: 0.9155 - val_accuracy: 0.8612\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.2692 - accuracy: 0.9354 - val_loss: 0.9213 - val_accuracy: 0.8611\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.2548 - accuracy: 0.9393 - val_loss: 0.9196 - val_accuracy: 0.8625\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.2431 - accuracy: 0.9419 - val_loss: 0.9243 - val_accuracy: 0.8623\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.2308 - accuracy: 0.9451 - val_loss: 0.9226 - val_accuracy: 0.8633\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.2198 - accuracy: 0.9477 - val_loss: 0.9278 - val_accuracy: 0.8632\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.2092 - accuracy: 0.9494 - val_loss: 0.9296 - val_accuracy: 0.8628\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.1986 - accuracy: 0.9519 - val_loss: 0.9295 - val_accuracy: 0.8643\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.1889 - accuracy: 0.9540 - val_loss: 0.9328 - val_accuracy: 0.8642\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.1814 - accuracy: 0.9553 - val_loss: 0.9395 - val_accuracy: 0.8640\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.1716 - accuracy: 0.9573 - val_loss: 0.9473 - val_accuracy: 0.8638\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.1637 - accuracy: 0.9589 - val_loss: 0.9485 - val_accuracy: 0.8645\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.1570 - accuracy: 0.9604 - val_loss: 0.9500 - val_accuracy: 0.8647\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.1505 - accuracy: 0.9614 - val_loss: 0.9578 - val_accuracy: 0.8644\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.1437 - accuracy: 0.9625 - val_loss: 0.9596 - val_accuracy: 0.8647\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.1378 - accuracy: 0.9636 - val_loss: 0.9678 - val_accuracy: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1daca8d750>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model_no_att.fit(x=[train_source_input_data, train_target_input_data],\n",
    "                      y=train_target_output_data,\n",
    "                      validation_data=([test_source_input_data, test_target_input_data], \n",
    "                                       test_target_output_data\n",
    "                                       ),\n",
    "                      epochs=50\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSmVtP2oXGAc"
   },
   "source": [
    "#### 1.c The Seq2seq model with Attention\n",
    "\n",
    "All we need to do is add an attention layer that ceates a context vector for each decoder position. We will then simply concatenate these corresponding context vectors with the output of the LSTM layer in order to predict the translation tokens one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aPu2HDtfGKvK"
   },
   "outputs": [],
   "source": [
    "### Translation Model ###\n",
    "\n",
    "source_input_with_att = tf.keras.layers.Input(shape=(max_encoder_seq_length,), \n",
    "                                     dtype='int64',\n",
    "                                     name='source_input_with_att')\n",
    "target_input_with_att = tf.keras.layers.Input(shape=(max_decoder_seq_length,), \n",
    "                                     dtype='int64',\n",
    "                                     name='target_input_with_att')\n",
    "\n",
    "source_embedding_layer_with_att = tf.keras.layers.Embedding(input_dim=encode_vocab_size,\n",
    "                                              output_dim=embed_dim, \n",
    "                                              name='source_embedding_layer_with_att')\n",
    "\n",
    "target_embeddings_layer_with_att  = tf.keras.layers.Embedding(input_dim=decode_vocab_size,\n",
    "                                              output_dim=embed_dim, \n",
    "                                              name='target_embedding_layer_with_att')\n",
    "\n",
    "\n",
    "source_embeddings_with_att = source_embedding_layer_with_att(source_input_with_att)\n",
    "target_embeddings_with_att = target_embeddings_layer_with_att(target_input_with_att)\n",
    "\n",
    "encoder_lstm_layer_with_att = tf.keras.layers.LSTM(embed_dim, return_sequences=True, return_state=True, name='encoder_lstm_layer_with_att')\n",
    "encoder_out_with_att, encoder_state_h_with_att, encoder_state_c_with_att = encoder_lstm_layer_with_att(source_embeddings_with_att)\n",
    "\n",
    "\n",
    "decoder_lstm_layer_with_att = tf.keras.layers.LSTM(embed_dim, return_sequences=True, return_state=False, name='decoder_lstm_layer_with_att')\n",
    "decoder_lstm_out_with_att = decoder_lstm_layer_with_att(target_embeddings_with_att, [encoder_state_h_with_att, encoder_state_c_with_att])\n",
    "\n",
    "\n",
    "attention_context_vectors = tf.keras.layers.Attention(name='attention_layer')([decoder_lstm_out_with_att, encoder_out_with_att])\n",
    "\n",
    "\n",
    "concat_decode_out_with_att = tf.keras.layers.Concatenate(axis=-1, name='concat_layer_with_att')([decoder_lstm_out_with_att, attention_context_vectors])\n",
    "\n",
    "target_classification_with_att = tf.keras.layers.Dense(decode_vocab_size, \n",
    "                                              activation='softmax', \n",
    "                                              name='classification_with_att')(concat_decode_out_with_att)\n",
    "\n",
    "\n",
    "translation_model_with_att = tf.keras.models.Model(inputs=[source_input_with_att, target_input_with_att], outputs=[target_classification_with_att])\n",
    "\n",
    "\n",
    "translation_model_with_att.compile(optimizer=\"Adam\",\n",
    "                          loss='sparse_categorical_crossentropy', \n",
    "                          metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYjG5lVPNBsU",
    "outputId": "46a4f630-d143-4398-c95b-4f6791cfa7d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 6s 13ms/step - loss: 2.3550 - accuracy: 0.6841 - val_loss: 1.5984 - val_accuracy: 0.7649\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 1.4830 - accuracy: 0.7699 - val_loss: 1.4514 - val_accuracy: 0.7709\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 1.3534 - accuracy: 0.7810 - val_loss: 1.3419 - val_accuracy: 0.7894\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 1.2349 - accuracy: 0.7953 - val_loss: 1.2629 - val_accuracy: 0.8010\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 1.1398 - accuracy: 0.8069 - val_loss: 1.1958 - val_accuracy: 0.8087\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 1.0635 - accuracy: 0.8139 - val_loss: 1.1547 - val_accuracy: 0.8151\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.9961 - accuracy: 0.8214 - val_loss: 1.1122 - val_accuracy: 0.8203\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.9338 - accuracy: 0.8293 - val_loss: 1.0803 - val_accuracy: 0.8271\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.8743 - accuracy: 0.8374 - val_loss: 1.0479 - val_accuracy: 0.8329\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.8176 - accuracy: 0.8444 - val_loss: 1.0277 - val_accuracy: 0.8367\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.7632 - accuracy: 0.8512 - val_loss: 1.0022 - val_accuracy: 0.8424\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.7125 - accuracy: 0.8575 - val_loss: 0.9770 - val_accuracy: 0.8442\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.6641 - accuracy: 0.8631 - val_loss: 0.9612 - val_accuracy: 0.8476\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.6183 - accuracy: 0.8698 - val_loss: 0.9459 - val_accuracy: 0.8500\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.5751 - accuracy: 0.8756 - val_loss: 0.9328 - val_accuracy: 0.8543\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.5353 - accuracy: 0.8821 - val_loss: 0.9230 - val_accuracy: 0.8550\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.4971 - accuracy: 0.8886 - val_loss: 0.9133 - val_accuracy: 0.8582\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.4607 - accuracy: 0.8952 - val_loss: 0.9059 - val_accuracy: 0.8602\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.4267 - accuracy: 0.9014 - val_loss: 0.9014 - val_accuracy: 0.8614\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.3964 - accuracy: 0.9070 - val_loss: 0.8997 - val_accuracy: 0.8628\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.3671 - accuracy: 0.9128 - val_loss: 0.8910 - val_accuracy: 0.8647\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.3399 - accuracy: 0.9189 - val_loss: 0.8852 - val_accuracy: 0.8670\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.3150 - accuracy: 0.9239 - val_loss: 0.8854 - val_accuracy: 0.8669\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.2923 - accuracy: 0.9290 - val_loss: 0.8884 - val_accuracy: 0.8683\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.2714 - accuracy: 0.9331 - val_loss: 0.8859 - val_accuracy: 0.8686\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.2527 - accuracy: 0.9372 - val_loss: 0.8865 - val_accuracy: 0.8700\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.2359 - accuracy: 0.9407 - val_loss: 0.8876 - val_accuracy: 0.8687\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.2190 - accuracy: 0.9448 - val_loss: 0.8917 - val_accuracy: 0.8699\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.2036 - accuracy: 0.9486 - val_loss: 0.8979 - val_accuracy: 0.8705\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.1909 - accuracy: 0.9506 - val_loss: 0.8971 - val_accuracy: 0.8710\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.1790 - accuracy: 0.9529 - val_loss: 0.9023 - val_accuracy: 0.8711\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.1672 - accuracy: 0.9551 - val_loss: 0.9075 - val_accuracy: 0.8716\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.1579 - accuracy: 0.9570 - val_loss: 0.9086 - val_accuracy: 0.8719\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.1481 - accuracy: 0.9596 - val_loss: 0.9144 - val_accuracy: 0.8714\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.1403 - accuracy: 0.9605 - val_loss: 0.9221 - val_accuracy: 0.8720\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.1332 - accuracy: 0.9621 - val_loss: 0.9317 - val_accuracy: 0.8717\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.1267 - accuracy: 0.9631 - val_loss: 0.9349 - val_accuracy: 0.8736\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.1201 - accuracy: 0.9649 - val_loss: 0.9447 - val_accuracy: 0.8729\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.1148 - accuracy: 0.9655 - val_loss: 0.9433 - val_accuracy: 0.8726\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.1087 - accuracy: 0.9664 - val_loss: 0.9476 - val_accuracy: 0.8730\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.1049 - accuracy: 0.9672 - val_loss: 0.9583 - val_accuracy: 0.8734\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.1010 - accuracy: 0.9680 - val_loss: 0.9650 - val_accuracy: 0.8733\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.0974 - accuracy: 0.9680 - val_loss: 0.9683 - val_accuracy: 0.8724\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.0934 - accuracy: 0.9689 - val_loss: 0.9704 - val_accuracy: 0.8736\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.0899 - accuracy: 0.9698 - val_loss: 0.9806 - val_accuracy: 0.8739\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.0875 - accuracy: 0.9702 - val_loss: 0.9865 - val_accuracy: 0.8728\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.0856 - accuracy: 0.9705 - val_loss: 0.9936 - val_accuracy: 0.8729\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.0822 - accuracy: 0.9707 - val_loss: 0.9924 - val_accuracy: 0.8722\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.0808 - accuracy: 0.9708 - val_loss: 0.9963 - val_accuracy: 0.8722\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.0778 - accuracy: 0.9714 - val_loss: 1.0051 - val_accuracy: 0.8728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1dc5e76b10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model_with_att.fit(x=[train_source_input_data, train_target_input_data],\n",
    "                      y=train_target_output_data,\n",
    "                      validation_data=([test_source_input_data, test_target_input_data], \n",
    "                                       test_target_output_data\n",
    "                                       ),\n",
    "                      epochs=50\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz6en_QG6Mnw"
   },
   "source": [
    "Validation accuracy is about 3/4 of a percentage point better. Nice.\n",
    "\n",
    "**Question:** Why do you think the benefit is not larger?\n",
    "\n",
    "## 2. T5\n",
    "\n",
    "Now we turn to text generation with transformers. \n",
    "\n",
    "Let's play a bit with Huggingface's (Large) implementation of T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZP02F_4E2Ngq",
    "outputId": "8764dedb-924f-4d3a-b933-75881413603c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (TFSharedEmbeddings)  multiple                 32899072  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  302040576 \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  402728448 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 737,668,096\n",
      "Trainable params: 737,668,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:169: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "\n",
    "t5_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukA3CNS6ZsZt"
   },
   "source": [
    "737 m trainable parameters. Quite a lot. \n",
    "\n",
    "Let's create a short text to use as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vGxjgkL96Lh3"
   },
   "outputs": [],
   "source": [
    "ARTICLE = ( \"Oh boy, what a lengthy and cumbersome excercise this was. I had to look into every detail, check everything twice,\\\n",
    "         and then compare to prior results. Because of this tediousness and extra work my homework was 2 days late.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-tEGC7MZz9F"
   },
   "source": [
    "Next, for T5 to work we need to specify the task and include it in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TgFCtCMP2TAo"
   },
   "outputs": [],
   "source": [
    "t5_input_text = \"summarize: \" + ARTICLE\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoR7GwLsZ96P"
   },
   "source": [
    "Now we will first generate a summary without and further specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BooaIspD2YHx",
    "outputId": "29803d1f-a59d-4a19-8c0a-1910221b36a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['homework was a lengthy and cumbersome excercise . because of this tedious']\n"
     ]
    }
   ],
   "source": [
    "# Generate Summary\n",
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'])\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHlbG7UOaEnr"
   },
   "source": [
    "Oh boy. Not great. But let's get more sophisticated and prespribe a min length and use Beamsearch: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cl2LvRRz2cH1",
    "outputId": "3acfc901-98f4-4327-f791-d2cd7a5347c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i had to look into every detail, check everything twice and compare with prior results. because of this tediousness my homework was 2 days late!']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                    num_beams=3,\n",
    "                                    no_repeat_ngram_size=1,\n",
    "                                    min_length=20,\n",
    "                                    max_length=40)\n",
    "                             \n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5-eCY8DaQTM"
   },
   "source": [
    "That is a bit better! \n",
    "\n",
    "Lastly, can it translate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "guJVGlqg5LWM"
   },
   "outputs": [],
   "source": [
    "t5_input_text = \"translate English to German: \" + ARTICLE\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CB6pE72B5PKb",
    "outputId": "462bfc15-a2eb-4eda-9309-189027e6c735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ich habe es mir sehr schwer gemacht, diese Aufgabe zu bewltigen.']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                    num_beams=3,\n",
    "                                    no_repeat_ngram_size=1,\n",
    "                                    min_length=20,\n",
    "                                    max_length=40)\n",
    "                             \n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1C3xL6ma5xC"
   },
   "source": [
    "Hmm... language fluency is very good. But the system shortened things a lot. A shorter example maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "SY93muRb5RiI"
   },
   "outputs": [],
   "source": [
    "t5_input_text = \"translate English to German: That was really not very good today; it was too difficult to solve.\"\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDsF31UZbK1I",
    "outputId": "3ec76408-a1ee-42f5-f678-3efa2bf45b0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Das war heute wirklich nicht sehr gut; es ist zu schwierig, die Sache aufzulsen.']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                    num_beams=3,\n",
    "                                    no_repeat_ngram_size=1,\n",
    "                                    min_length=20,\n",
    "                                    max_length=40)\n",
    "                             \n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJy_BapabMYQ"
   },
   "source": [
    "That is not bad, though some mistakes are there."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lesson_5_Text_Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
