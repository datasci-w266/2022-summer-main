{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E9wN1a_EUHv"
   },
   "source": [
    "### Training NLP models in Colab without running out of RAM\n",
    "\n",
    "This notebook focuses on some techniques you can use to avoid running out of memory, when working with a lot of data and large models used for NLP tasks.\n",
    "\n",
    "The task we'll work on is textual entailment, using the [Stanford Natural Language Inference (SNLI) dataset](https://nlp.stanford.edu/projects/snli/). We'll build a fairly simple classification model, using a pre-trained BERT model. (Some of the code is inspired by [this Keras example for SNLI classification](https://keras.io/examples/nlp/semantic_similarity_with_bert/).)\n",
    "\n",
    "The main focus of this notebook is not on the task or model architecture, but on how to load part of your data at a time while you train, and save model checkpoints as you go. You should be able to run the notebook on the free tier of Google Colab. (There is a point where it will run out of RAM, for demonstration, but that is noted in the comments.)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2022-summer-main/blob/master/materials/walkthrough_notebooks/keras_with_limited_ram/keras_training_with_limited_ram.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTb7WQBJETaa"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GgTstPMhUCcB"
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# These auto classes load the right type of tokenizer and model based on a model name\n",
    "from transformers import AutoTokenizer, TFAutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIoEALljGPSb"
   },
   "source": [
    "We'll start by downloading the data, using curl in bash to save it to the local disk space for the Colab notebook. You might have your data in Google Drive instead; later we'll mount a Drive folder to this notebook so that we can save our model someplace more permanent, but you can move that step up if you need to load data from Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98eb0b67",
    "outputId": "57135d3d-acf1-4d96-d1b7-f0ed3c8f4c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 11.1M  100 11.1M    0     0  28.9M      0 --:--:-- --:--:-- --:--:-- 28.9M\n",
      "SNLI_Corpus/\n",
      "SNLI_Corpus/snli_1.0_dev.csv\n",
      "SNLI_Corpus/snli_1.0_train.csv\n",
      "SNLI_Corpus/snli_1.0_test.csv\n"
     ]
    }
   ],
   "source": [
    "!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n",
    "!tar -xvzf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "084fa28f",
    "outputId": "52a342b1-e3f1-466f-b0c5-f507c274c9d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snli_1.0_dev.csv  snli_1.0_test.csv  snli_1.0_train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls SNLI_Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXir3qflG6mf"
   },
   "source": [
    "First let's read in the entire train and dev datasets. It looks like we have about 550k training examples, and 10k dev examples (which we'll use for validation). Just loading those short sentence pairs doesn't take a lot of RAM, but it will be too much to process with a BERT model. (You can see how much RAM and Disk space you're using by looking in the upper right corner of the notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGgVdcybUQM7",
    "outputId": "77287981-cc08-4502-dec8-f466b8fd4779"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550152, 3), (10000, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filename = 'SNLI_Corpus/snli_1.0_train.csv'\n",
    "dev_filename = 'SNLI_Corpus/snli_1.0_dev.csv'\n",
    "\n",
    "df_train = pd.read_csv(train_filename)\n",
    "df_dev = pd.read_csv(dev_filename)\n",
    "\n",
    "df_train.shape, df_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCbmUSvNHU83"
   },
   "source": [
    "Let's define some functions that we'll need to preprocess the data and build our classification model. First, we'll tokenize the sentence pairs using the pretrained BERT tokenizer. Second, we need to convert the three label classes from strings to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Sz6xMq-CdF2V"
   },
   "outputs": [],
   "source": [
    "label_dict = {'neutral': 0, 'entailment': 1, 'contradiction': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "00bee460"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(sentence_pairs, label_strs, tokenizer, max_length=128):\n",
    "    # With BERT tokenizer's batch_encode_plus, sentence pairs are\n",
    "    # encoded together and separated by [SEP] token.\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        sentence_pairs,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "    # Extract encoded features and labels, add to corresponding lists\n",
    "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "    # Convert string labels into numbered categories\n",
    "    labels = np.array([label_dict[label] if label in label_dict else 0\n",
    "                       for label in label_strs])\n",
    "    \n",
    "    return [input_ids, attention_masks, token_type_ids], labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqVIMi8CqJ2N"
   },
   "source": [
    "For the model, we'll construct a fairly simple classification model on top of the pretrained BERT model. Since we're freezing the full BERT model, it doesn't work very well for this classification problem to just use the pre-trained CLS token output as our vector representing the full input that we want to classify. (It would probably work better if we unfroze some BERT layers to fine-tune that CLS token.) Instead, we'll add one more attention layer on top of the full sequence of contextualized token vectors that we get out of BERT, so that we can train that attention layer to pay attention to the tokens that are most useful for this entailment task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "336d30d4"
   },
   "outputs": [],
   "source": [
    "def build_snli_model(bert_model, max_length=128, hidden_dim=256):\n",
    "    input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='input_ids')\n",
    "    attention_masks = layers.Input(shape=(max_length), dtype=tf.int32, name='attention_masks')\n",
    "    token_type_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='token_type_ids')\n",
    "    \n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
    "    sequence_output = bert_output.last_hidden_state\n",
    "\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=4, key_dim=100)(sequence_output, sequence_output)\n",
    "    max_pool = layers.GlobalMaxPooling1D()(attn_output)\n",
    "    dropout_output = layers.Dropout(0.3)(max_pool)\n",
    "    final_output = layers.Dense(3, activation=\"softmax\")(dropout_output)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_masks, token_type_ids],\n",
    "                                  outputs=[final_output])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhUr-JYBH26m"
   },
   "source": [
    "Ok, let's load the pretrained tokenizer and model, build the classification model, and preprocess our data to get ready to train. We'll freeze the BERT model layers for the live demo, so we keep the pre-trained weights rather than fine-tuning. (We will still be training the new layers we add on top of BERT for classification.) If you set the last line in the cell below to True, and train further on your task, you'll be fine-tuning the BERT model. It will take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82de9daa",
    "outputId": "e97d1636-0da4-4740-ecf1-283b02e15df6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model_name='bert-base-uncased'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = TFAutoModel.from_pretrained(bert_model_name)\n",
    "bert_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9u181p__jMYz",
    "outputId": "2db6fb99-fd1f-440e-8a94-512943c43bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n",
      "                                tentions(last_hidde               'token_type_ids[0][0]']         \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 128, 768)    1230768     ['tf_bert_model_1[0][0]',        \n",
      " eadAttention)                                                    'tf_bert_model_1[0][0]']        \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 768)         0           ['multi_head_attention_1[0][0]'] \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)           (None, 768)          0           ['global_max_pooling1d[0][0]']   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3)            2307        ['dropout_74[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,715,315\n",
      "Trainable params: 1,233,075\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_snli_model(bert_model, max_length=128, hidden_dim=256)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MaNJK72fa9m1"
   },
   "outputs": [],
   "source": [
    "# Tokenize the 1k of the dev examples to use for validation data first\n",
    "# (You can use more, but we'll just use 1k for the live demo)\n",
    "\n",
    "dev_sentence_pairs = df_dev[['sentence1', 'sentence2']].values[:1000].astype(str).tolist()\n",
    "dev_labels = df_dev['similarity'].values[:1000]\n",
    "\n",
    "dev_data = preprocess_data(\n",
    "    dev_sentence_pairs, dev_labels, tokenizer=bert_tokenizer, max_length=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "d8c98711"
   },
   "outputs": [],
   "source": [
    "# Now tokenize the 550k training examples ...\n",
    "# ONLY RUN THIS CELL THE FIRST TIME FOR DEMONSTRATION, IT MIGHT RUN OUT OF RAM\n",
    "\n",
    "sentence_pairs = df_train[['sentence1', 'sentence2']].values.astype(str).tolist()\n",
    "labels = df_train['similarity'].values\n",
    "\n",
    "train_data = preprocess_data(\n",
    "    sentence_pairs, labels, tokenizer=bert_tokenizer, max_length=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsHk4FBOWr3T"
   },
   "source": [
    "At some point when running the last cell, if you're using the free Collab tier, your notebook probably ran out of RAM and crashed. (If it didn't, you're on a Colab machine with more RAM; available resources may vary. But you may not be able to actually train the model with all of that data in memory.)\n",
    "\n",
    "Let's try again, but this time, we won't load all of our data at once. Connect the notebook again (it may have restarted on its own), and run most of the code above, but stop after tokenizing the dev data (which we'll keep for validation below). Don't tokenize the full 550k dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QnoXcWoabhsU"
   },
   "outputs": [],
   "source": [
    "# In case you loaded the full dataset above\n",
    "df_train = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyfiTM0LIUJt"
   },
   "source": [
    "We can define a [custom class called a data generator](https://medium.com/analytics-vidhya/write-your-own-custom-data-generator-for-tensorflow-keras-1252b64e41c3), that we will pass to model.fit instead of our full dataset. This data generator needs to implement methods for `__len__`, `__getitem__`, and `on_epoch_end`. In `__getitem__`, we'll write the code to get the next batch of data to train the model. We can write that function so it only loads and tokenizes the data needed for the next batch. In `on_epoch_end` we'll shuffle the order in which we plan to load data for the next epoch.\n",
    "\n",
    "We'll look at how to do this two ways. First, in the data we downloaded from SNLI, all of the training data is in one large CSV file. We can use the pandas `pd.read_csv` method, which includes options to skip certain rows of data and only load a certain number. We won't just want to load a consecutive chunk each time, because we'll want to shuffle the rows. The `read_csv` method doesn't quite have an option to specify individual row indices to load, but we can specify a list of row indices to skip, so that's what we'll do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4b82e1a2"
   },
   "outputs": [],
   "source": [
    "class SNLIDataGeneratorFromFile(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 n_examples,\n",
    "                 data_filename,\n",
    "                 max_length=128,\n",
    "                 batch_size=32,\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_examples = n_examples\n",
    "        self.data_filename = data_filename\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
    "        self.row_order = np.arange(1, self.n_examples+1)\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        # NOTE: USING REDUCED BATCHES PER EPOCH TO SPEED UP THE LIVE DEMO\n",
    "        # For normal use, this line should be:\n",
    "        # return self.n_examples // self.batch_size\n",
    "        return 100\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_start = idx * self.batch_size\n",
    "        batch_end = (idx + 1) * self.batch_size\n",
    "\n",
    "        # Indices to skip are the ones in the shuffled row_order before and\n",
    "        # after the chunk we'll use for this batch\n",
    "        batch_idx_skip = self.row_order[:batch_start] + self.row_order[batch_end:]\n",
    "        df = pd.read_csv(self.data_filename, skiprows=batch_idx_skip)\n",
    "        \n",
    "        sentence_pairs = df[['sentence1', 'sentence2']].values.astype(str).tolist()\n",
    "        labels = df['similarity'].values\n",
    "        \n",
    "        batch_data = preprocess_data(\n",
    "            sentence_pairs,\n",
    "            labels,\n",
    "            self.tokenizer,\n",
    "            self.max_length\n",
    "        )\n",
    "\n",
    "        return batch_data\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.row_order = list(np.random.permutation(self.row_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "os_CX135Yd3J"
   },
   "outputs": [],
   "source": [
    "train_data_generator = SNLIDataGeneratorFromFile(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    n_examples=550152,\n",
    "    data_filename='SNLI_Corpus/snli_1.0_train.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQgtbBM7A-IZ"
   },
   "source": [
    "One more thing. It's going to take a while to train our model (even with a GPU, which we'll need to use). Colab resources are free but shared, there are usage limits and our notebook might time out especially when using a GPU for a while. So we should periodically save a copy of our trained model as we go. Later, we can load the model that we saved and keep training it further.\n",
    "\n",
    "At this point, we probably do want to mount a Google Drive folder, because we won't want to save our checkpoints just to temporary Colab disk space. If our notebook disconnects, we'll lose those files. The next cell mounts your Drive folder, then for demonstration I'm showing my (the instructor's) UC Berkeley Drive path to where I'm storing files for this semester's class. You'll want to edit that for your Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wsjKJ7cA0-J",
    "outputId": "dcfa81be-e9cf-462d-aad7-616e985cffa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmI9rlUJA1bC",
    "outputId": "6fad8ff0-7312-481a-e1e8-89dc3613d36c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras_training_with_limited_ram.ipynb  TensorFlow.ipynb\n",
      "model_checkpoints\t\t       Text_classification.ipynb\n",
      "NNBasics.ipynb\n"
     ]
    }
   ],
   "source": [
    "# CHANGE THIS TO THE PATH IN YOUR OWN DRIVE WHERE YOU WANT TO SAVE CHECKPOINTS\n",
    "\n",
    "!ls drive/MyDrive/ISchool/MIDS/W266/2022_Summer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TY4RwR5_syuG"
   },
   "source": [
    "Keras provides a handy [ModelCheckpoint](https://keras.io/api/callbacks/model_checkpoint/) class that we can pass into .fit as a callback. By default, it'll save a checkpoint of the model at the end of each epoch of training.\n",
    "\n",
    "We can choose to save the whole model or just the weights (i.e. the model parameters that we've trained so far). And we'll specify the destination filepath (we can include formatting options to have different filenames for each epoch and loss).\n",
    "\n",
    "(Other options: you can also choose to only save the best performing model each time, based on a performance metric you choose. But we'll save after every epoch here so that we can see the resulting files in the live demo.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yNjbSiWN_Xce"
   },
   "outputs": [],
   "source": [
    "# CHANGE checkpoint_dir TO THE PATH IN YOUR OWN DRIVE WHERE YOU WANT TO SAVE CHECKPOINTS\n",
    "\n",
    "checkpoint_dir = 'drive/MyDrive/ISchool/MIDS/W266/2022_Summer/model_checkpoints/'\n",
    "checkpoint_filepath = checkpoint_dir + 'weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hCG0YzqKb2w"
   },
   "source": [
    "Now we're ready to train our model. We'll call `model.fit`, but instead of passing in an array of data, we'll pass in our data generator. And we'll include the model checkpoint callback, to save the weights after each epoch.\n",
    "\n",
    "The next cell may take a couple hours to run per epoch on the full dataset. The ETA in the running output can be very useful to estimate how long it will take your model to train (and whether you need to interrupt it and make adjustments to be able to make progress).\n",
    "\n",
    "Note: For demonstration purposes during the live demo, we're only using a few batches of data per epoch, so that we can see it train and save checkpoints. In the SNLIDataGeneratorFromFile code above, see the comments on the `__len__` method. Change that code in the `__len__` method back to the correct batches per epoch to see how long it takes to train on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUv93OhYWe1Z",
    "outputId": "6a93e27e-2965-48ca-9e3c-3085ed74dbdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 67s 547ms/step - loss: 1.0672 - accuracy: 0.4509 - val_loss: 0.8885 - val_accuracy: 0.5950\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 0.8511 - accuracy: 0.6250 - val_loss: 0.7110 - val_accuracy: 0.6900\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 53s 525ms/step - loss: 0.7747 - accuracy: 0.6709 - val_loss: 0.6968 - val_accuracy: 0.6940\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 53s 527ms/step - loss: 0.7149 - accuracy: 0.6938 - val_loss: 0.6288 - val_accuracy: 0.7400\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 53s 532ms/step - loss: 0.7140 - accuracy: 0.7025 - val_loss: 0.6259 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe909af0dd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data_generator, validation_data=dev_data, epochs=5,\n",
    "          callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3wuQDUfMbhs"
   },
   "source": [
    "If we need to pick up where we left off, we can load the weights that we saved into our model and then call `model.fit` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "HEQ3GSc1WfZE"
   },
   "outputs": [],
   "source": [
    "# CHANGE checkpoint_filepath TO EXACT NAME OF A SAVED CHECKPOINT YOU WANT TO LOAD\n",
    "\n",
    "checkpoint_filepath = checkpoint_dir + 'weights.05-0.75.hdf5'\n",
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zkf9re_jEFfg"
   },
   "source": [
    "What if our data is stored in many small files? We might want to only load one file of data at a time, and randomly shuffle the order in which we load files from the data folder for each training epoch.\n",
    "\n",
    "Just to demonstrate that option, we'll simulate having our data in multiple files. We'll use the same dataset, but read the full dataset once and write it to a bunch of csv files of 256 rows each. (You won't typically do this if your data starts out all in one file, in which case you can just read select rows like we did above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "d9da19f8"
   },
   "outputs": [],
   "source": [
    "!mkdir SNLI_Corpus/train_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7cb18474"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_filename)\n",
    "for i in range(0, 550152, 256):\n",
    "    df_train[i:i+256].to_csv('SNLI_Corpus/train_files/train_data_%d.csv' % i, index=False)\n",
    "\n",
    "df_train = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d891751",
    "outputId": "7827aec5-6eba-47f0-cae7-5a3857d6a291"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'SNLI_Corpus/train_files/'\n",
    "data_filenames = os.listdir(data_dir)\n",
    "len(data_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqGhJG3g9oLv"
   },
   "source": [
    "Now we have 2150 separate csv files of training data, and we'll only want to load one or a few at a time to train our model. Our data files have 256 rows in each, and we'll only use 32 examples per batch, so we'll usually only load one file at a time and tokenize part of it for the next batch of data.\n",
    "\n",
    "The code below will work whether your files are larger or smaller than one batch, though. We'll keep track of which rows we've already used from the current file and take the next rows for a new batch, so we might run past the current file and load another file to fill up the rest of the batch.\n",
    "\n",
    "In your own project, you might have data files that are smaller or larger, so we've made the code somewhat flexible so that you can see how to load just enough files to get the next batch of data that you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "5c004e85"
   },
   "outputs": [],
   "source": [
    "class SNLIDataGeneratorFromDir(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 n_examples,\n",
    "                 data_dir,\n",
    "                 examples_per_file,\n",
    "                 max_length=128,\n",
    "                 batch_size=32,\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_examples = n_examples\n",
    "        self.data_dir = data_dir\n",
    "        self.examples_per_file = examples_per_file\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.filename_order = os.listdir(self.data_dir)\n",
    "        self.next_file_i = 0\n",
    "        self.next_row_i = 0\n",
    "        \n",
    "        # Call on_epoch_end to shuffle data at start\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        # NOTE: USING REDUCED BATCHES PER EPOCH TO SPEED UP THE LIVE DEMO\n",
    "        # For normal use, this line should be:\n",
    "        # return self.n_examples // self.batch_size\n",
    "        return 100\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        files_to_load = (self.batch_size // self.examples_per_file) + 1\n",
    "        \n",
    "        sentence_pairs = []\n",
    "        labels = []\n",
    "        \n",
    "        for file_i in range(self.next_file_i, self.next_file_i + files_to_load):\n",
    "            filepath = os.path.join(self.data_dir, self.filename_order[file_i])\n",
    "            df = pd.read_csv(filepath)\n",
    "            n_remaining = self.batch_size - len(sentence_pairs)\n",
    "            \n",
    "            start = self.next_row_i\n",
    "            end = self.next_row_i + n_remaining\n",
    "            curr_sent_pairs = df[['sentence1', 'sentence2']].values[start:end]\n",
    "            sentence_pairs.extend(curr_sent_pairs.tolist())\n",
    "            \n",
    "            curr_labels = df['similarity'].values[start:end]\n",
    "            labels.extend(curr_labels.tolist())\n",
    "            \n",
    "            if end < len(df):\n",
    "                self.next_file_i = file_i\n",
    "                self.next_row_i = end\n",
    "            else:\n",
    "                self.next_file_i = file_i + 1\n",
    "                self.next_row_i = 0\n",
    "                \n",
    "            if len(sentence_pairs) >= self.batch_size:\n",
    "                break\n",
    "            \n",
    "        batch_data = preprocess_data(\n",
    "            sentence_pairs,\n",
    "            labels,\n",
    "            self.tokenizer,\n",
    "            self.max_length\n",
    "        )\n",
    "        return batch_data\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.next_file_i = 0\n",
    "        self.next_row_i = 0\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.filename_order = np.random.permutation(self.filename_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9lYIQ76mCjxY"
   },
   "outputs": [],
   "source": [
    "train_data_generator = SNLIDataGeneratorFromDir(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    n_examples=550152,\n",
    "    data_dir=data_dir,\n",
    "    examples_per_file=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9hs1MsUCj8g",
    "outputId": "148a76e8-97d3-400e-905c-99c1f7a6568e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 40s 402ms/step - loss: 0.7206 - accuracy: 0.7019 - val_loss: 0.6189 - val_accuracy: 0.7450\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 41s 414ms/step - loss: 0.7082 - accuracy: 0.7044 - val_loss: 0.6315 - val_accuracy: 0.7280\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 41s 414ms/step - loss: 0.7004 - accuracy: 0.7103 - val_loss: 0.6246 - val_accuracy: 0.7410\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.6849 - accuracy: 0.7113 - val_loss: 0.6209 - val_accuracy: 0.7380\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.6583 - accuracy: 0.7209 - val_loss: 0.6317 - val_accuracy: 0.7420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9e516ec90>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data_generator, validation_data=dev_data, epochs=5,\n",
    "          callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeFL23iYCkGs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d38357ad"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "keras_training_with_limited_ram.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
